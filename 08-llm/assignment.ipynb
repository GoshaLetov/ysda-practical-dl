{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8bdb2c71",
      "metadata": {
        "id": "8bdb2c71",
        "papermill": {
          "duration": 0.012817,
          "end_time": "2023-06-21T18:03:30.544219",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.531402",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Practice: Large Language Models and Their Implications\n",
        "\n",
        "<!-- ![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg) -->\n",
        "![img](https://i.imgur.com/QGYa2J8.jpeg)\n",
        "\n",
        "In this notebook, you're gonna play with some of the largest language models on the Internet.\n",
        "\n",
        "_Based on works of: Tim Dettmers, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin, Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f272f448",
      "metadata": {
        "id": "f272f448",
        "papermill": {
          "duration": 0.011961,
          "end_time": "2023-06-21T18:03:30.568790",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.556829",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Part 1: prompt engineering (3 points total)\n",
        "\n",
        "In the assignment, we'll use public APIs that host the 100B+ models for inference. Your task is to prompt-engineer the model into solving a few tasks for you.\n",
        "\n",
        "\n",
        "__Which API?__ You are free to use any publicly available API. Here's a few options:\n",
        "\n",
        "- BLOOM API - [bigscience/bloom](https://huggingface.co/bigscience/bloom) (on the right; recommended)\n",
        "- OpenAI API (via VPN) - [openai.com/api](https://openai.com/api/)\n",
        "- AI21 Jurrasic API - [ai21.com](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\n",
        "\n",
        "These APIs may require you to create a (free) account on their platform. Please note that some APIs also have paid subscriptions. __You do not need to pay them__, this assignment was designed to be solved using free-tier subscriptions. If no APIs work for you, you can also solve these tasks with the 6.7B model that you will find later in this notebook - but this will make the tasks somewhat harder.\n",
        "\n",
        "__Quests:__ you will need to solve 4 problems. For each one, please attach a short __description__ of your solution and a __screenshot__ from the API you use. _[If you use python APIs, show your python code with outputs]_\n",
        "\n",
        "__Example:__ Tony is talking to Darth Vader ([BLOOM API](https://huggingface.co/bigscience/bloom)). Black text is written manually, blue text is generated.\n",
        "<hr>\n",
        "\n",
        "![img](https://i.imgur.com/a1QhKF7.png)\n",
        "<hr>\n",
        "\n",
        "__It is fine to roll back a few times,__ e.g. in the example above, the model first generated Vader lines twice in a row, and we rolled that back. However, if you need more than 1-2 rollbacks per session, you should probably try a different prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7673897b",
      "metadata": {
        "id": "7673897b",
        "papermill": {
          "duration": 0.012675,
          "end_time": "2023-06-21T18:03:30.594477",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.581802",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "__Task 1 (1 pt):__ arange a conversation between any two of the following:\n",
        "\n",
        "- a celebrity or politician of your choice\n",
        "- any fictional character (except Darth Vader)\n",
        "- yourself\n",
        "\n",
        "Compare two setups: a) you prompt with character names only b) you supply additional information (see example)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123a3169",
      "metadata": {
        "id": "123a3169",
        "papermill": {
          "duration": 0.012264,
          "end_time": "2023-06-21T18:03:30.618960",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.606696",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/yn6sNhM/2023-06-19-21-59-07.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929b6786",
      "metadata": {
        "id": "929b6786",
        "papermill": {
          "duration": 0.012249,
          "end_time": "2023-06-21T18:03:30.643126",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.630877",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/bLv5xmy/2023-06-19-22-11-49.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24757eae",
      "metadata": {
        "id": "24757eae",
        "papermill": {
          "duration": 0.012044,
          "end_time": "2023-06-21T18:03:30.667274",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.655230",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "__Please choose task 2a or 2b (1pt)__ depending on your model (you can do both, but you will be awarded points for one of these two tasks).\n",
        "\n",
        "__Task 2a: (for BLOOM or other multilingual model)__ zero-shot translation. Take the first verse of [Edgar Allan Poe's \"Raven\"](https://www.poetryfoundation.org/poems/48860/the-raven) and __translate it into French.__ (You are free to use any other text of at least the same size)\n",
        "\n",
        "Original text: ```\n",
        "Once upon a midnight dreary, while I pondered, weak and weary,\n",
        "Over many a quaint and curious volume of forgotten lore—\n",
        "    While I nodded, nearly napping, suddenly there came a tapping,\n",
        "As of some one gently rapping, rapping at my chamber door.\n",
        "“’Tis some visitor,” I muttered, “tapping at my chamber door—\n",
        "            Only this and nothing more.”\n",
        "```\n",
        "\n",
        "Verify your translation by converting french back into english using a public machine translation service.\n",
        "\n",
        "__Task 2b: (non-BLOOM):__ toxicity classification for [SetFit/toxic_conversations](https://huggingface.co/datasets/SetFit/toxic_conversations). Make the model solve binary classification (toxic vs not toxic) in the few shot mode. For few-shot examples, use 2-3 toxic and 2-3 non-toxic non-toxic examples. Measure accuracy on at least 25 samples. You may need to try several different prompts before you find the one that works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eca8571",
      "metadata": {
        "id": "7eca8571",
        "papermill": {
          "duration": 0.012111,
          "end_time": "2023-06-21T18:03:30.691796",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.679685",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/d4xV7K6/2023-06-19-22-19-25.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88fbb3c5",
      "metadata": {
        "id": "88fbb3c5",
        "papermill": {
          "duration": 0.012044,
          "end_time": "2023-06-21T18:03:30.715952",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.703908",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/RPQFGK8/2023-06-19-22-20-25.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd3df69",
      "metadata": {
        "id": "0dd3df69",
        "papermill": {
          "duration": 0.012003,
          "end_time": "2023-06-21T18:03:30.739936",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.727933",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "__Task 3 (1pt):__ create a prompt and few-shot examples tha make the model __change the gender pronouns__ of the main actor in a given sentence in any direction of your choice. E.g. the doctor took off _his_ mask <-> the doctor took of _her_ mask.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "722bed46",
      "metadata": {
        "id": "722bed46",
        "papermill": {
          "duration": 0.011815,
          "end_time": "2023-06-21T18:03:30.763738",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.751923",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/VSd1j2Q/2023-06-19-22-28-19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcba5e2f",
      "metadata": {
        "id": "fcba5e2f",
        "papermill": {
          "duration": 0.011675,
          "end_time": "2023-06-21T18:03:30.787671",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.775996",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "__Task 4 (1pt):__ write a prompt and supply examples such that the model would __convert imperial units to metric units__ (miles -> kilometers; mph -> kph). More specifically, the model should rewrite a given sentence and replace all imperial units with their metric equivalents. After it works with basic distances and speed, try to find complicated examples where it does *not* work.\n",
        "\n",
        "Please note that 1 mile is not equal to 1 km :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf9a526",
      "metadata": {
        "id": "bcf9a526",
        "papermill": {
          "duration": 0.011744,
          "end_time": "2023-06-21T18:03:30.811926",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.800182",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![img](https://i.ibb.co/WxVX6L0/2023-06-19-22-43-05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0373977c",
      "metadata": {
        "id": "0373977c",
        "papermill": {
          "duration": 0.011903,
          "end_time": "2023-06-21T18:03:30.835734",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.823831",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Part 2: local inference\n",
        "\n",
        "Now, let's try and load the strongest model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n",
        "\n",
        "Our best candidates are the smaller versions of the best performing open source models:\n",
        "- 7 Bn parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n",
        "- 7 Bn parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n",
        "- 6.7 Bn parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n",
        "\n",
        "Beware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n",
        "\n",
        "It's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n",
        "<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "714a0fce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:03:30.861565Z",
          "iopub.status.busy": "2023-06-21T18:03:30.860881Z",
          "iopub.status.idle": "2023-06-21T18:03:34.551488Z",
          "shell.execute_reply": "2023-06-21T18:03:34.550490Z"
        },
        "id": "714a0fce",
        "outputId": "f359c887-d197-41df-afc1-bf1f2df76fd9",
        "papermill": {
          "duration": 3.706735,
          "end_time": "2023-06-21T18:03:34.554331",
          "exception": false,
          "start_time": "2023-06-21T18:03:30.847596",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version 2.0.1+cu118 imported OK. \n",
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.get_device_capability() < (7, 5):\n",
        "    raise ValueError(f\"You got a GPU with capability {torch.cuda.get_device_capability()}, need at least (7, 5)\")\n",
        "else:\n",
        "    print(f\"Pytorch version {torch.__version__} imported OK. \\ndevice = {device}\")\n",
        "\n",
        "# Note: this code requires a Turing GPU or newer. Good: T4, RTX 20xx/30xx, A100/Axx; Bad: K80, P100, V100\n",
        "# Colab gives you T4. If you get older GPUs, please wait or switch to a new account (don't use both at the same time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588d64d1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:03:34.580546Z",
          "iopub.status.busy": "2023-06-21T18:03:34.580026Z",
          "iopub.status.idle": "2023-06-21T18:04:01.421362Z",
          "shell.execute_reply": "2023-06-21T18:04:01.420161Z"
        },
        "id": "588d64d1",
        "papermill": {
          "duration": 26.856888,
          "end_time": "2023-06-21T18:04:01.423579",
          "exception": false,
          "start_time": "2023-06-21T18:03:34.566691",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install --quiet bitsandbytes==0.39.0 transformers==4.29.2 datasets==2.12.0 accelerate==0.19.0 sentencepiece==0.1.99 einops==0.6.1\n",
        "\n",
        "from IPython.display import clear_output; clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62896af",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:04:01.450083Z",
          "iopub.status.busy": "2023-06-21T18:04:01.449737Z",
          "iopub.status.idle": "2023-06-21T18:04:04.259672Z",
          "shell.execute_reply": "2023-06-21T18:04:04.258412Z"
        },
        "id": "e62896af",
        "outputId": "a7917db8-426a-4e34-a2f8-d9acf15ff2df",
        "papermill": {
          "duration": 2.826476,
          "end_time": "2023-06-21T18:04:04.262671",
          "exception": false,
          "start_time": "2023-06-21T18:04:01.436195",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                    0.19.0\n",
            "bitsandbytes                  0.39.0\n",
            "datasets                      2.12.0\n",
            "einops                        0.6.1\n",
            "sentencepiece                 0.1.99\n",
            "tensorflow-datasets           4.9.2\n",
            "torch                         2.0.1+cu118\n",
            "torchaudio                    2.0.2+cu118\n",
            "torchdata                     0.6.1\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.2\n",
            "torchvision                   0.15.2+cu118\n",
            "transformers                  4.29.2\n",
            "vega-datasets                 0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep -e \"bitsandbytes\" -e \"transformers\" -e datasets -e torch -e accelerate -e sentencepiece -e einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc5d439",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:04:04.291550Z",
          "iopub.status.busy": "2023-06-21T18:04:04.290503Z",
          "iopub.status.idle": "2023-06-21T18:04:07.449691Z",
          "shell.execute_reply": "2023-06-21T18:04:07.448785Z"
        },
        "id": "5dc5d439",
        "papermill": {
          "duration": 3.17589,
          "end_time": "2023-06-21T18:04:07.452121",
          "exception": false,
          "start_time": "2023-06-21T18:04:04.276231",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from typing import Tuple\n",
        "from typing import List\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import LlamaTokenizer\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from tqdm.auto import trange\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97366edd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:04:07.513084Z",
          "iopub.status.busy": "2023-06-21T18:04:07.512798Z",
          "iopub.status.idle": "2023-06-21T18:08:06.891691Z",
          "shell.execute_reply": "2023-06-21T18:08:06.888386Z"
        },
        "id": "97366edd",
        "papermill": {
          "duration": 239.395711,
          "end_time": "2023-06-21T18:08:06.894807",
          "exception": false,
          "start_time": "2023-06-21T18:04:07.499096",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_name = 'decapoda-research/llama-7b-hf'  # published in 03-2023 model best in class among openly available\n",
        "\n",
        "# note: the flags `torch_dtype`, `low_cpu_mem_usage`, `offload_state_dict`, `load_in_8bit` slow down the code to save RAM; remove them if you have >32GB RAM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b344ef8d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:06.939581Z",
          "iopub.status.busy": "2023-06-21T18:08:06.939187Z",
          "iopub.status.idle": "2023-06-21T18:08:07.591179Z",
          "shell.execute_reply": "2023-06-21T18:08:07.590376Z"
        },
        "id": "b344ef8d",
        "papermill": {
          "duration": 0.679305,
          "end_time": "2023-06-21T18:08:07.593674",
          "exception": false,
          "start_time": "2023-06-21T18:08:06.914369",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#  loading Llama tokenizer...\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d79fc12",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:07.632654Z",
          "iopub.status.busy": "2023-06-21T18:08:07.632328Z",
          "iopub.status.idle": "2023-06-21T18:08:07.650610Z",
          "shell.execute_reply": "2023-06-21T18:08:07.649784Z"
        },
        "id": "1d79fc12",
        "papermill": {
          "duration": 0.040506,
          "end_time": "2023-06-21T18:08:07.653069",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.612563",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "for module in model.modules():\n",
        "    if isinstance(module, bnb.nn.Linear8bitLt):\n",
        "        module.state.memory_efficient_backward = True\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # freeze the model - train adapters later\n",
        "    if param.ndim == 1:\n",
        "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations, comment out if you have >32GB RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e52c30",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:07.693010Z",
          "iopub.status.busy": "2023-06-21T18:08:07.692169Z",
          "iopub.status.idle": "2023-06-21T18:08:07.697974Z",
          "shell.execute_reply": "2023-06-21T18:08:07.697157Z"
        },
        "id": "d8e52c30",
        "papermill": {
          "duration": 0.029819,
          "end_time": "2023-06-21T18:08:07.701929",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.672110",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# code to save from RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
        "\n",
        "class AddInputGrad(nn.Sequential):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x).requires_grad_(True)\n",
        "\n",
        "model.model.embed_tokens = AddInputGrad(model.model.embed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438ae964",
      "metadata": {
        "id": "438ae964",
        "papermill": {
          "duration": 0.019653,
          "end_time": "2023-06-21T18:08:07.740815",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.721162",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### Text generation\n",
        "\n",
        "**Comparison of strategies for language model text generation:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Documentation references:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b324bc82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:07.780293Z",
          "iopub.status.busy": "2023-06-21T18:08:07.779977Z",
          "iopub.status.idle": "2023-06-21T18:08:07.784351Z",
          "shell.execute_reply": "2023-06-21T18:08:07.783481Z"
        },
        "id": "b324bc82",
        "papermill": {
          "duration": 0.028335,
          "end_time": "2023-06-21T18:08:07.788427",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.760092",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from warnings import filterwarnings; filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3176e4f1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:07.829611Z",
          "iopub.status.busy": "2023-06-21T18:08:07.829236Z",
          "iopub.status.idle": "2023-06-21T18:08:07.857471Z",
          "shell.execute_reply": "2023-06-21T18:08:07.856665Z"
        },
        "id": "3176e4f1",
        "outputId": "e7d7508b-8f51-4a46-eb3a-ab846e555cfd",
        "papermill": {
          "duration": 0.052195,
          "end_time": "2023-06-21T18:08:07.859921",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.807726",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids : tensor([    0,   450,   937, 10943, 14436], device='cuda:0')\n",
            "attention_mask : tensor([1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "prompt = 'The first discovered martian lifeform looks like'\n",
        "max_new_tokens = 64\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for key, value in batch.items():\n",
        "    print(key, ':', value[0, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6377c4",
      "metadata": {
        "id": "fb6377c4",
        "papermill": {
          "duration": 0.017978,
          "end_time": "2023-06-21T18:08:07.896246",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.878268",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### greedy generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772777b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:07.928630Z",
          "iopub.status.busy": "2023-06-21T18:08:07.928364Z",
          "iopub.status.idle": "2023-06-21T18:08:26.620548Z",
          "shell.execute_reply": "2023-06-21T18:08:26.618368Z"
        },
        "id": "772777b5",
        "outputId": "c719713d-abd3-4abc-9b9e-c21da787189d",
        "papermill": {
          "duration": 18.708417,
          "end_time": "2023-06-21T18:08:26.622803",
          "exception": false,
          "start_time": "2023-06-21T18:08:07.914386",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like a bacteria, but it is not. It is a virus. It is a virus that infects bacteria. It is a virus that infects bacteria and then uses the bacteria as a host to replicate. It is a virus that infects bacteria and\n",
            "CPU times: user 16.3 s, sys: 400 ms, total: 16.7 s\n",
            "Wall time: 18.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ac8f95",
      "metadata": {
        "id": "81ac8f95",
        "papermill": {
          "duration": 0.014594,
          "end_time": "2023-06-21T18:08:26.654663",
          "exception": false,
          "start_time": "2023-06-21T18:08:26.640069",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### detailed code for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6378c9d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:26.682867Z",
          "iopub.status.busy": "2023-06-21T18:08:26.682544Z",
          "iopub.status.idle": "2023-06-21T18:08:30.706981Z",
          "shell.execute_reply": "2023-06-21T18:08:30.705980Z"
        },
        "id": "f6378c9d",
        "outputId": "d0e147d7-bd06-414d-a255-f28c0b7bd4c0",
        "papermill": {
          "duration": 4.041147,
          "end_time": "2023-06-21T18:08:30.709303",
          "exception": false,
          "start_time": "2023-06-21T18:08:26.668156",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moscow is the capital of \n",
            "\n",
            "▁Russia   : 0.8309 \n",
            "▁the      : 0.0749 \n",
            "▁Russian  : 0.0131 \n",
            "▁and      : 0.0092 \n",
            "▁Moscow   : 0.0064 \n",
            "\t\t\t Russia\n",
            ".         : 0.6163 \n",
            "▁and      : 0.1214 \n",
            ",         : 0.0808 \n",
            "and       : 0.0332 \n",
            "▁Moscow   : 0.0167 \n",
            "\t\t\t .\n",
            "<0x0A>    : 0.1929 \n",
            "▁It       : 0.1756 \n",
            "▁Moscow   : 0.1217 \n",
            "▁The      : 0.0538 \n",
            "▁Russia   : 0.0397 \n",
            "\t\t\t <0x0A>\n",
            "▁<        : 0.1153 \n",
            "<0x0A>    : 0.1104 \n",
            "▁Moscow   : 0.0762 \n",
            "▁</       : 0.0455 \n",
            "M         : 0.0367 \n",
            "\t\t\t <\n",
            "0         : 0.9651 \n",
            "1         : 0.0090 \n",
            "<         : 0.0044 \n",
            "2         : 0.0021 \n",
            "3         : 0.0016 \n",
            "\t\t\t 0\n",
            "x         : 0.9977 \n",
            ">         : 0.0004 \n",
            "1         : 0.0003 \n",
            "A         : 0.0001 \n",
            "0         : 0.0001 \n",
            "\t\t\t x\n",
            "0         : 0.9778 \n",
            "1         : 0.0085 \n",
            "2         : 0.0061 \n",
            "A         : 0.0022 \n",
            "3         : 0.0015 \n",
            "\t\t\t 0\n",
            "A         : 0.8817 \n",
            "B         : 0.0381 \n",
            "0         : 0.0108 \n",
            "D         : 0.0094 \n",
            "C         : 0.0088 \n",
            "\t\t\t A\n",
            ">         : 0.8094 \n",
            "><        : 0.1353 \n",
            ">(        : 0.0121 \n",
            "></       : 0.0097 \n",
            ">[        : 0.0086 \n",
            "\t\t\t >\n",
            "<0x0A>    : 0.0798 \n",
            "M         : 0.0691 \n",
            "Russ      : 0.0680 \n",
            "The       : 0.0416 \n",
            "▁Moscow   : 0.0413 \n",
            "\t\t\t <0x0A>\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"Moscow is the capital of\"\n",
        "# prompt1 = \"Skippy, a young android, likes to dream about electric\"\n",
        "\n",
        "print(prompt1, '\\n')\n",
        "\n",
        "voc = tokenizer.get_vocab()\n",
        "voc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n",
        "past_key_values = None\n",
        "n = 5\n",
        "temperature = 1.0  # change in range 0.1 .. 10 to test effects in sampling generation mode\n",
        "\n",
        "for i in range(10):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        batch1 = tokenizer(prompt1, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "        outputs = model.forward(**batch1, use_cache=True,)\n",
        "        logits = outputs.logits[0, -1, :] / temperature\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # GREEDY GENERATION\n",
        "        next_token_id = logits.argmax(-1)\n",
        "\n",
        "        # SAMPLING GENERATION\n",
        "        # next_token_id = torch.multinomial(probs, num_samples=1)  # uncomment for sampling generation\n",
        "\n",
        "        next_token = tokenizer.decode(next_token_id)\n",
        "        prompt1 += next_token\n",
        "\n",
        "        # next_token = voc_rev[next_token_id.item()]  # uncomment to show raw tokens\n",
        "\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        # cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        top_tokens = sorted_indices[:n]\n",
        "        for t, p in zip (top_tokens, sorted_probs):\n",
        "            t = voc_rev[t.item()]\n",
        "            # t = tokenizer.decode([t])\n",
        "            print(f\"{t:<10}: {p:.4f} \")\n",
        "\n",
        "        print('\\t\\t\\t', f\"{next_token}\", end='\\n', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d277baa1",
      "metadata": {
        "id": "d277baa1",
        "papermill": {
          "duration": 0.014048,
          "end_time": "2023-06-21T18:08:30.738116",
          "exception": false,
          "start_time": "2023-06-21T18:08:30.724068",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### sampling generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1996a8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:30.767953Z",
          "iopub.status.busy": "2023-06-21T18:08:30.767644Z",
          "iopub.status.idle": "2023-06-21T18:08:45.167250Z",
          "shell.execute_reply": "2023-06-21T18:08:45.166283Z"
        },
        "id": "fb1996a8",
        "outputId": "71ce15c8-e4db-4fde-d89a-43f96a14f36d",
        "papermill": {
          "duration": 14.417519,
          "end_time": "2023-06-21T18:08:45.169594",
          "exception": false,
          "start_time": "2023-06-21T18:08:30.752075",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like a worm, but it only grows 1mm/year, no way this would be classified as life, and no way it would be discovered by now. The third lifeform is in the middle of the desert on Mars, the only thing is that in a 4000km (25\n"
          ]
        }
      ],
      "source": [
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19389737",
      "metadata": {
        "id": "19389737",
        "papermill": {
          "duration": 0.016212,
          "end_time": "2023-06-21T18:08:45.200797",
          "exception": false,
          "start_time": "2023-06-21T18:08:45.184585",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### sampling generation with temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fc6f0c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:45.233275Z",
          "iopub.status.busy": "2023-06-21T18:08:45.232087Z",
          "iopub.status.idle": "2023-06-21T18:08:58.828441Z",
          "shell.execute_reply": "2023-06-21T18:08:58.826972Z"
        },
        "id": "33fc6f0c",
        "outputId": "795b8567-a3a2-4796-f38d-3a35e06e5bce",
        "papermill": {
          "duration": 13.614887,
          "end_time": "2023-06-21T18:08:58.830785",
          "exception": false,
          "start_time": "2023-06-21T18:08:45.215898",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like… This isn’tequite perfect, it’ll still have the weirdness-for-oddittydime and all of Mars…\n",
            "Sorry for taking this longer than any of my friends, any more. I’m…going toredispersein. Good by! Have more questions from\n"
          ]
        }
      ],
      "source": [
        "# moderate temperature makes generated text more diverse\n",
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens, do_sample=True, temperature = 2.0)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ad665a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:08:58.863529Z",
          "iopub.status.busy": "2023-06-21T18:08:58.863126Z",
          "iopub.status.idle": "2023-06-21T18:09:13.806080Z",
          "shell.execute_reply": "2023-06-21T18:09:13.804290Z"
        },
        "id": "f9ad665a",
        "outputId": "dae1426c-6dff-4ce8-b0c3-47437923938d",
        "papermill": {
          "duration": 14.962923,
          "end_time": "2023-06-21T18:09:13.808677",
          "exception": false,
          "start_time": "2023-06-21T18:08:58.845754",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like Earth amo [...]...\n",
            "Cameron J. - It must sucks at drowns so a bajilian. You will make his mother, even... He would never see... It has already died a little in a hundred billion stars it just didn\" know... Oh yes a.\n",
            "Galax\n"
          ]
        }
      ],
      "source": [
        "# tool high a temperature makes the model generate unrelated text\n",
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens, do_sample=True, temperature = 8.0)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1ce46f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:09:13.840431Z",
          "iopub.status.busy": "2023-06-21T18:09:13.840106Z",
          "iopub.status.idle": "2023-06-21T18:09:27.742842Z",
          "shell.execute_reply": "2023-06-21T18:09:27.741844Z"
        },
        "id": "fb1ce46f",
        "outputId": "2e53aa6b-a6e3-4657-e0ea-a89c998846fc",
        "papermill": {
          "duration": 13.921425,
          "end_time": "2023-06-21T18:09:27.745405",
          "exception": false,
          "start_time": "2023-06-21T18:09:13.823980",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like a cross between a bacterium and a spider, and is called the \"groom\" because of its resemblance to a bride's gown. It is a single-celled organism that lives in the soil of Mars. The \"groom\" is a member of the Archae\n"
          ]
        }
      ],
      "source": [
        "# Low temperature brings results closer to greedy search output\n",
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens, do_sample=True, temperature = 0.3)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79eb366e",
      "metadata": {
        "id": "79eb366e",
        "papermill": {
          "duration": 0.014629,
          "end_time": "2023-06-21T18:09:27.775029",
          "exception": false,
          "start_time": "2023-06-21T18:09:27.760400",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10bb4ae4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:09:27.807015Z",
          "iopub.status.busy": "2023-06-21T18:09:27.806256Z",
          "iopub.status.idle": "2023-06-21T18:10:01.080405Z",
          "shell.execute_reply": "2023-06-21T18:10:01.079412Z"
        },
        "id": "10bb4ae4",
        "outputId": "d73b40b1-d689-4232-9ce3-6799fb24b610",
        "papermill": {
          "duration": 33.308372,
          "end_time": "2023-06-21T18:10:01.098068",
          "exception": false,
          "start_time": "2023-06-21T18:09:27.789696",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>The first discovered martian lifeform looks like a bacterium, but it’s not. It’s an archaea, a type of single-celled organism that’s more closely related to bacteria than it is to eukaryotes, such as plants and animals. It’s also the first archaea to be discovered on another planet. The discovery was made by a team of researchers led by David Des Marais of the Jet Propulsion Laboratory (JPL) in Pasadena, California. The team’s findings are published in this week’s issue of the Proceedings of the National Academy of Sciences\n"
          ]
        }
      ],
      "source": [
        "# beam search is best for high quality longer generation\n",
        "with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens * 2, num_beams=4)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab538a10",
      "metadata": {
        "id": "ab538a10",
        "papermill": {
          "duration": 0.015967,
          "end_time": "2023-06-21T18:10:01.130610",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.114643",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "**Task 5: write code for nucleus sampling generation (2 points)**:\n",
        "\n",
        "Use the `nucleus_sampling()` template below. Look at the detailed generation code above for inspiration.\n",
        "\n",
        "**Bonus task: write code for beam search (extra 2 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f4c780",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:01.162158Z",
          "iopub.status.busy": "2023-06-21T18:10:01.161305Z",
          "iopub.status.idle": "2023-06-21T18:10:01.170328Z",
          "shell.execute_reply": "2023-06-21T18:10:01.169455Z"
        },
        "id": "92f4c780",
        "papermill": {
          "duration": 0.0271,
          "end_time": "2023-06-21T18:10:01.172546",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.145446",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(model, tokenizer, prompt: str, prob: float = 0.5) -> Tuple[str, List[str]]:\n",
        "    \"\"\"generates the next token from the nucleus of tokens with cumulative probability up to param:prob\"\"\"\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "        logits = model.forward(**batch, use_cache=True).logits[0, -1, :]\n",
        "\n",
        "    p_next = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    p_next_sorted, indexes_sorted = torch.sort(p_next, dim=-1, descending=True)\n",
        "\n",
        "    nucleus = torch.cumsum(p_next_sorted, dim=-1) >= prob\n",
        "    indexes = indexes_sorted[:nucleus.to(dtype=torch.int8).argmax() + 1]\n",
        "\n",
        "    possible_tokens = [tokenizer.decode(index) for index in indexes]\n",
        "    sampled_token = tokenizer.decode(indexes[torch.multinomial(input=p_next[indexes], num_samples=1)])\n",
        "\n",
        "    return sampled_token, possible_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b0695c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:01.204918Z",
          "iopub.status.busy": "2023-06-21T18:10:01.204282Z",
          "iopub.status.idle": "2023-06-21T18:10:01.796234Z",
          "shell.execute_reply": "2023-06-21T18:10:01.794983Z"
        },
        "id": "b6b0695c",
        "outputId": "2c6ceec5-2fff-4ef1-e47e-75dff104dcb8",
        "papermill": {
          "duration": 0.610471,
          "end_time": "2023-06-21T18:10:01.798409",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.187938",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elbrus is the highest mountain ['mountain', 'peak', 'vol']\n",
            "Large language models can learn to generate ['generate', 'perform', 'predict', 'translate']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from typing import Tuple, List\n",
        "\n",
        "# Tests for nucleus sampling\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "test_prompt = \"Elbrus is the highest\"\n",
        "next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.85) # corrected from 0.9 to 0.85\n",
        "print(test_prompt, next_token, possible_tokens)\n",
        "assert next_token == 'mountain'\n",
        "assert {'mountain', 'peak'}.difference(set(possible_tokens)) == set()\n",
        "assert 3 <= len(possible_tokens) <= 4\n",
        "\n",
        "test_prompt = \"Large language models can learn to\"\n",
        "next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.4)\n",
        "print(test_prompt, next_token, possible_tokens)\n",
        "assert next_token == 'generate'\n",
        "assert {'generate', 'perform', 'translate'}.difference(set(possible_tokens)) == set()\n",
        "assert 3 <= len(possible_tokens) <= 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kV2Mryr1sHHN"
      },
      "id": "kV2Mryr1sHHN"
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, tokenizer, prompt: str, num_beams: int = 4, max_new_tokens: int = 10) -> Tuple[str, List[str]]:\n",
        "    \"\"\"generates the next param:max_new_tokens tokens with beam search algorithm\"\"\"\n",
        "\n",
        "    prompt_tensor = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        logits = model(**prompt_tensor).logits[:, -1, :]\n",
        "        probas = logits.softmax(dim=-1)\n",
        "\n",
        "    topk_scores, topk_indices = torch.topk(probas, k=num_beams)\n",
        "    topk_scores, topk_indices = topk_scores.squeeze(), topk_indices.squeeze()\n",
        "\n",
        "    # Initialize the list of generated sequences and their scores\n",
        "    generated_sequences = [prompt + tokenizer.decode(topk_indices[i]) for i in range(num_beams)]\n",
        "    sequences_scores = topk_scores\n",
        "\n",
        "    for _ in range(max_new_tokens - 1):\n",
        "        # Initialize the list of new sequences and their scores\n",
        "        new_sequences = []\n",
        "        new_scores = []\n",
        "\n",
        "        # Generate continuations for each sequence\n",
        "        for sequence, score in zip(generated_sequences, sequences_scores):\n",
        "            sequence_tensor = tokenizer(sequence, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "            # Generate the next token probabilities using the model\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(**sequence_tensor).logits[:, -1, :]\n",
        "                probas = logits.softmax(dim=-1)\n",
        "\n",
        "            # Get the top-k tokens and their corresponding scores\n",
        "            topk_scores, topk_indices = torch.topk(probas, k=num_beams)\n",
        "            topk_scores, topk_indices = topk_scores.squeeze(), topk_indices.squeeze()\n",
        "\n",
        "            # Expand the current sequence and score for each beam\n",
        "            for beam_score, beam_token in zip(topk_scores, topk_indices):\n",
        "                new_sequence = sequence + tokenizer.decode(beam_token.item())\n",
        "                new_sequences.append(new_sequence)\n",
        "                new_scores.append(score + beam_score.item())\n",
        "\n",
        "        # Select the top-k sequences with the highest scores\n",
        "        topk_scores, topk_indices = torch.topk(torch.tensor(new_scores), k=num_beams)\n",
        "\n",
        "        # Update the generated sequences and scores\n",
        "        generated_sequences = [new_sequences[i] for i in topk_indices]\n",
        "\n",
        "    return generated_sequences[sequences_scores.argmax()]\n"
      ],
      "metadata": {
        "id": "PsvyX_nnsGyp"
      },
      "id": "PsvyX_nnsGyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"Elbrus is the highest \"\n",
        "beam_search(model, tokenizer, test_prompt, max_new_tokens=10, num_beams=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "48Trg3g2sIFs",
        "outputId": "830a7d3f-ed9e-47b2-f92c-6a7cbeca709c"
      },
      "id": "48Trg3g2sIFs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elbrus is the highest 18,000-footpeakin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast():\n",
        "    batch = tokenizer(test_prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=10, num_beams=4)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0].detach().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NiqYK5XsJTT",
        "outputId": "f9092814-2557-417c-d3ef-51b4950a73d8"
      },
      "id": "7NiqYK5XsJTT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>Elbrus is the highest 18,000-foot peak in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f861d5",
      "metadata": {
        "id": "f2f861d5",
        "papermill": {
          "duration": 0.016608,
          "end_time": "2023-06-21T18:10:01.830756",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.814148",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Part 3: parameter-efficient fine-tuning\n",
        "\n",
        "__Task 8: Parameter-efficient finetuning with LoRA (1 point)__\n",
        "\n",
        "Since the model barely fits into memory, we won't be able to train it with conventional fine-tuning. Instead, you can use low-rank adapters based on [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with attention projection matrices,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb89fb2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:01.863298Z",
          "iopub.status.busy": "2023-06-21T18:10:01.862308Z",
          "iopub.status.idle": "2023-06-21T18:10:01.869963Z",
          "shell.execute_reply": "2023-06-21T18:10:01.869094Z"
        },
        "id": "3fb89fb2",
        "papermill": {
          "duration": 0.026,
          "end_time": "2023-06-21T18:10:01.871969",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.845969",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        W = self.module(input)\n",
        "        A = input @ self.adapter_A @ self.adapter_B\n",
        "        return W + A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ca61c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:01.905328Z",
          "iopub.status.busy": "2023-06-21T18:10:01.905035Z",
          "iopub.status.idle": "2023-06-21T18:10:01.978696Z",
          "shell.execute_reply": "2023-06-21T18:10:01.977742Z"
        },
        "id": "14ca61c6",
        "outputId": "d6ba393e-d17a-488a-a8ce-8d3da08105a2",
        "papermill": {
          "duration": 0.093915,
          "end_time": "2023-06-21T18:10:01.980913",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.886998",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb9457d",
      "metadata": {
        "id": "3cb9457d",
        "papermill": {
          "duration": 0.015976,
          "end_time": "2023-06-21T18:10:02.014773",
          "exception": false,
          "start_time": "2023-06-21T18:10:01.998797",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b88171e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:02.048948Z",
          "iopub.status.busy": "2023-06-21T18:10:02.047891Z",
          "iopub.status.idle": "2023-06-21T18:10:02.072795Z",
          "shell.execute_reply": "2023-06-21T18:10:02.070965Z"
        },
        "id": "4b88171e",
        "papermill": {
          "duration": 0.044217,
          "end_time": "2023-06-21T18:10:02.074870",
          "exception": false,
          "start_time": "2023-06-21T18:10:02.030653",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f7aba5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:02.108773Z",
          "iopub.status.busy": "2023-06-21T18:10:02.108496Z",
          "iopub.status.idle": "2023-06-21T18:10:03.571823Z",
          "shell.execute_reply": "2023-06-21T18:10:03.570498Z"
        },
        "id": "26f7aba5",
        "papermill": {
          "duration": 1.484169,
          "end_time": "2023-06-21T18:10:03.574413",
          "exception": false,
          "start_time": "2023-06-21T18:10:02.090244",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
        "\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d9ca70",
      "metadata": {
        "id": "40d9ca70",
        "papermill": {
          "duration": 0.01728,
          "end_time": "2023-06-21T18:10:03.607484",
          "exception": false,
          "start_time": "2023-06-21T18:10:03.590204",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a14fca6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:03.640843Z",
          "iopub.status.busy": "2023-06-21T18:10:03.640511Z",
          "iopub.status.idle": "2023-06-21T18:10:06.292822Z",
          "shell.execute_reply": "2023-06-21T18:10:06.291588Z"
        },
        "id": "1a14fca6",
        "papermill": {
          "duration": 2.672126,
          "end_time": "2023-06-21T18:10:06.295556",
          "exception": false,
          "start_time": "2023-06-21T18:10:03.623430",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "e2af60959abc413d849dfb38b948a6cf",
            "69d41bd0d3eb40d4be766ed13f7c6c0f",
            "8e8665ef8f75471c84852392b0867a23",
            "097c988f97d54c25ad0d49a084a00d21",
            "2867245d4a624f2289b5eb23744daaea",
            "be6c64dfdd084113965c60695f556024",
            "2bde111aef8b4000ae0d2299b3db727d"
          ]
        },
        "outputId": "70d4d57d-d9c1-451e-c9d9-3e45226e4f6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2af60959abc413d849dfb38b948a6cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/Abirate--english_quotes to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69d41bd0d3eb40d4be766ed13f7c6c0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e8665ef8f75471c84852392b0867a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "097c988f97d54c25ad0d49a084a00d21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2867245d4a624f2289b5eb23744daaea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be6c64dfdd084113965c60695f556024",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bde111aef8b4000ae0d2299b3db727d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8064ef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:06.337091Z",
          "iopub.status.busy": "2023-06-21T18:10:06.336772Z",
          "iopub.status.idle": "2023-06-21T18:10:19.476406Z",
          "shell.execute_reply": "2023-06-21T18:10:19.475481Z"
        },
        "id": "ce8064ef",
        "outputId": "bf3d1beb-27dd-4a43-8d33-9dd535a21b7c",
        "papermill": {
          "duration": 13.163855,
          "end_time": "2023-06-21T18:10:19.478746",
          "exception": false,
          "start_time": "2023-06-21T18:10:06.314891",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.930600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.634100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.248200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.438600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.861500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# checking if the model can learn. Change max_steps for proper training\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, train_dataset=data['train'],\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4, gradient_accumulation_steps=1,\n",
        "        warmup_steps=250, max_steps=5, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=None),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "_ = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed458a49",
      "metadata": {
        "id": "ed458a49",
        "papermill": {
          "duration": 0.017834,
          "end_time": "2023-06-21T18:10:19.514625",
          "exception": false,
          "start_time": "2023-06-21T18:10:19.496791",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### Now you train the model (3 points)\n",
        "\n",
        "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
        "\n",
        "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
        "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
        "* __short lines:__ please take the first 512 characters of each line\n",
        "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
        "   - extra adapter on lm_head\n",
        "   - extra adapter on MLP components (mlp.*)\n",
        "   - trainable input embeddings (requires tweaking memory usage)\n",
        "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00215cc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:19.554762Z",
          "iopub.status.busy": "2023-06-21T18:10:19.554383Z",
          "iopub.status.idle": "2023-06-21T18:10:40.756230Z",
          "shell.execute_reply": "2023-06-21T18:10:40.755281Z"
        },
        "id": "c00215cc",
        "papermill": {
          "duration": 21.223707,
          "end_time": "2023-06-21T18:10:40.758360",
          "exception": false,
          "start_time": "2023-06-21T18:10:19.534653",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset(\n",
        "    \"codeparrot/codeparrot-clean\", data_files={\"file-000000000001.json.gz\"}\n",
        ")['train'].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa99087c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:40.794856Z",
          "iopub.status.busy": "2023-06-21T18:10:40.794572Z",
          "iopub.status.idle": "2023-06-21T18:10:40.799290Z",
          "shell.execute_reply": "2023-06-21T18:10:40.798212Z"
        },
        "papermill": {
          "duration": 0.025232,
          "end_time": "2023-06-21T18:10:40.801585",
          "exception": false,
          "start_time": "2023-06-21T18:10:40.776353",
          "status": "completed"
        },
        "tags": [],
        "id": "fa99087c"
      },
      "outputs": [],
      "source": [
        "def flatten(lists: list) -> list:\n",
        "    return [content for sublist in lists for content in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009f4085",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:40.837651Z",
          "iopub.status.busy": "2023-06-21T18:10:40.837379Z",
          "iopub.status.idle": "2023-06-21T18:10:43.657569Z",
          "shell.execute_reply": "2023-06-21T18:10:43.656540Z"
        },
        "papermill": {
          "duration": 2.841332,
          "end_time": "2023-06-21T18:10:43.660247",
          "exception": false,
          "start_time": "2023-06-21T18:10:40.818915",
          "status": "completed"
        },
        "tags": [],
        "id": "009f4085"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "contents = flatten(list(map(lambda code: code['content'].split('\\n'), train_data)))\n",
        "\n",
        "random.shuffle(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1042e94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:43.698254Z",
          "iopub.status.busy": "2023-06-21T18:10:43.697370Z",
          "iopub.status.idle": "2023-06-21T18:10:43.704029Z",
          "shell.execute_reply": "2023-06-21T18:10:43.703146Z"
        },
        "papermill": {
          "duration": 0.027684,
          "end_time": "2023-06-21T18:10:43.706099",
          "exception": false,
          "start_time": "2023-06-21T18:10:43.678415",
          "status": "completed"
        },
        "tags": [],
        "id": "a1042e94"
      },
      "outputs": [],
      "source": [
        "class PreprocessedPythonCode(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: dict):\n",
        "        super(PreprocessedPythonCode, self).__init__()\n",
        "        self.contents = contents\n",
        "\n",
        "    def __getitem__(self, index) -> None:\n",
        "        return tokenizer(self.contents[index], padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e72ffcba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:43.742698Z",
          "iopub.status.busy": "2023-06-21T18:10:43.741913Z",
          "iopub.status.idle": "2023-06-21T18:10:43.746864Z",
          "shell.execute_reply": "2023-06-21T18:10:43.745966Z"
        },
        "papermill": {
          "duration": 0.025228,
          "end_time": "2023-06-21T18:10:43.748759",
          "exception": false,
          "start_time": "2023-06-21T18:10:43.723531",
          "status": "completed"
        },
        "tags": [],
        "id": "e72ffcba"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_data = PreprocessedPythonCode(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56382a5a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:10:43.786166Z",
          "iopub.status.busy": "2023-06-21T18:10:43.785399Z",
          "iopub.status.idle": "2023-06-21T18:12:06.431227Z",
          "shell.execute_reply": "2023-06-21T18:12:06.429894Z"
        },
        "id": "56382a5a",
        "outputId": "9a7818b3-74e7-451f-960b-2edb1cf27644",
        "papermill": {
          "duration": 82.667047,
          "end_time": "2023-06-21T18:12:06.433266",
          "exception": false,
          "start_time": "2023-06-21T18:10:43.766219",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [01:22<00:00, 10.33s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "prompts_texts_before = []\n",
        "\n",
        "# generate baseline samples with the selected prompts before finetuning\n",
        "for prompt in tqdm(prompts):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False)\n",
        "        out = model.generate(**batch, max_new_tokens=32, num_beams=4)\n",
        "    prompts_texts_before.append(tokenizer.decode(out[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccafec35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:12:06.472739Z",
          "iopub.status.busy": "2023-06-21T18:12:06.471884Z",
          "iopub.status.idle": "2023-06-21T18:12:06.477244Z",
          "shell.execute_reply": "2023-06-21T18:12:06.476366Z"
        },
        "id": "ccafec35",
        "papermill": {
          "duration": 0.026755,
          "end_time": "2023-06-21T18:12:06.479230",
          "exception": false,
          "start_time": "2023-06-21T18:12:06.452475",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.lm_head = LoRALayer(model.lm_head, rank=lora_rank)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c250e4a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-21T18:12:06.517410Z",
          "iopub.status.busy": "2023-06-21T18:12:06.517101Z",
          "iopub.status.idle": "2023-06-22T00:08:50.863047Z",
          "shell.execute_reply": "2023-06-22T00:08:50.862009Z"
        },
        "id": "6c250e4a",
        "outputId": "de2a674d-f8cc-4bfc-f362-e0c85e61f4f8",
        "papermill": {
          "duration": 21404.368376,
          "end_time": "2023-06-22T00:08:50.865782",
          "exception": false,
          "start_time": "2023-06-21T18:12:06.497406",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 5:56:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.999000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.887200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.858000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=preprocessed_train_data,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=250, max_steps=500, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=100, output_dir='llama-7b-hf-python-code-ft', report_to=None),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "_ = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212f7aa9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-22T00:08:50.907594Z",
          "iopub.status.busy": "2023-06-22T00:08:50.907182Z",
          "iopub.status.idle": "2023-06-22T00:10:12.867555Z",
          "shell.execute_reply": "2023-06-22T00:10:12.866387Z"
        },
        "id": "212f7aa9",
        "outputId": "e726e43c-86f3-451e-f74f-04a3bce7b604",
        "papermill": {
          "duration": 81.982598,
          "end_time": "2023-06-22T00:10:12.869352",
          "exception": false,
          "start_time": "2023-06-22T00:08:50.886754",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [01:21<00:00, 10.24s/it]\n"
          ]
        }
      ],
      "source": [
        "prompts_texts_after = []\n",
        "\n",
        "for prompt in tqdm(prompts):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False)\n",
        "        out = model.generate(**batch, max_new_tokens=32, num_beams=4)\n",
        "    prompts_texts_after.append(tokenizer.decode(out[0].detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc54fffd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-22T00:10:12.911527Z",
          "iopub.status.busy": "2023-06-22T00:10:12.911195Z",
          "iopub.status.idle": "2023-06-22T00:10:12.920080Z",
          "shell.execute_reply": "2023-06-22T00:10:12.919098Z"
        },
        "id": "bc54fffd",
        "outputId": "98ef3751-3b0a-434d-efa9-131209fd0866",
        "papermill": {
          "duration": 0.03236,
          "end_time": "2023-06-22T00:10:12.922470",
          "exception": false,
          "start_time": "2023-06-22T00:10:12.890110",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>… … … … … … … … … … … … … … … … … …</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>    def __init__(self, *args, **kwa</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>import20222222222222222222222222222</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>import os.path as op  # pylint: dis</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>from from from from from from from </pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>from __future__ import absolute_imp</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>while you're at it.\n",
              "I don't know ab</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>while True:\r\n",
              "                    tr</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>try try to try to try to try to try</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>try:\r\n",
              "                    self.asse</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>if you want to use it.\n",
              "\n",
              "\\begin{code</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>if __name__ == '__main__':\r\n",
              "    uni</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>for the 2018-2019 school year is $1</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>for i in range(0, 100):\r\n",
              "          </pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>torch torch torch torch torch torch</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"><unk>torch.autograd.set_grad_enabled(Fal</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for prompt, text_before, text_after in zip(prompts, prompts_texts_before, prompts_texts_after):\n",
        "    # replace placeholders in the format() arguments\n",
        "    rows.append(row_template.format(prompt, text_before[:40], text_after[:40]))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1fe13e6",
      "metadata": {
        "id": "c1fe13e6",
        "papermill": {
          "duration": 0.019782,
          "end_time": "2023-06-22T00:10:12.962620",
          "exception": false,
          "start_time": "2023-06-22T00:10:12.942838",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "If you reach this: congratulations! you've completed everything in this practice session.\n",
        "\n",
        "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
        "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
        "\n",
        "\n",
        "\n",
        "#### Read more\n",
        "\n",
        "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
        "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
        "* A general library for different adapter types: https://adapterhub.ml/\n",
        "\n",
        "### [extra info] How to optimize for inference\n",
        "\n",
        "The code below converts training-optimized 8bit weights into inference-optimized layout. It should result in significantly faster inference in the same memory footprint.\n",
        "However, if you do this, you can no longer run training --\n",
        " there is no way to un-convert after the first optimized forward!\n",
        "\n",
        "```python\n",
        "model.config.use_cache = True\n",
        "for module in model.modules():\n",
        "    if isinstance(module, bnb.nn.Linear8bitLt):\n",
        "        module.state.memory_efficient_backward = False\n",
        "```\n",
        "\n",
        "### [extra info] Running other models.\n",
        "\n",
        "This notebook's code can run with other models of similar size. Most comparable are OPT-6.7b and Falcon-7B. They will require minor code tweaks:\n",
        "1) indicate the model name in `AutoModelForCausalLM.from_pretrained()`\n",
        "2) change tokenizer class to `AutoTokenizer`\n",
        "3) change code to force gradient_required:\n",
        "```\n",
        "        class AddInputGrad(nn.Sequential):\n",
        "            def forward(self, x):\n",
        "                return super().forward(x).requires_grad_(True)\n",
        "\n",
        "        if 'facebook/opt' in model_name:\n",
        "            model.model.decoder.project_in = lambda x: x.requires_grad_(True)\n",
        "        elif 'falcon' in model_name:\n",
        "            model.transformer.word_embeddings = AddInputGrad(model.transformer.word_embeddings)\n",
        "```\n",
        "\n",
        "4) change code to add Lora layers - need to refer to the transformer block components by their relevant names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7d4005",
      "metadata": {
        "id": "9f7d4005",
        "papermill": {
          "duration": 0.019856,
          "end_time": "2023-06-22T00:10:13.002196",
          "exception": false,
          "start_time": "2023-06-22T00:10:12.982340",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 22017.706329,
      "end_time": "2023-06-22T00:10:16.936681",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-06-21T18:03:19.230352",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}